import requests
import pandas as pd
from datetime import datetime
import time
import os

class ComprehensiveStackOverflowCollector:
    def __init__(self, api_key=None):
        self.base_url = "https://api.stackexchange.com/2.3"
        self.api_key = api_key
    
    def get_questions_by_tag(self, tag, from_date, to_date, max_pages=20):
        """ë‹¨ì¼ íƒœê·¸ë¡œ ì§ˆë¬¸ ìˆ˜ì§‘"""
        url = f"{self.base_url}/questions"
        
        params = {
            "site": "stackoverflow",
            "tagged": tag,
            "fromdate": int(from_date.timestamp()),
            "todate": int(to_date.timestamp()),
            "sort": "creation",
            "order": "desc",
            "pagesize": 100
        }
        
        if self.api_key:
            params["key"] = self.api_key
        
        all_questions = []
        page = 1
        
        print(f"\nğŸ” '{tag}' íƒœê·¸ ìˆ˜ì§‘ ì¤‘...")
        
        while page <= max_pages:
            params['page'] = page
            
            try:
                response = requests.get(url, params=params)
                
                if response.status_code != 200:
                    print(f"  âŒ ì—ëŸ¬: {response.status_code}")
                    break
                
                data = response.json()
                questions = data.get('items', [])
                
                if not questions:
                    break
                
                all_questions.extend(questions)
                
                quota_remaining = data.get('quota_remaining', 0)
                has_more = data.get('has_more', False)
                
                print(f"  [{page}/{max_pages}] ğŸ“¦ {len(all_questions)}ê°œ | ë‚¨ì€ ìš”ì²­: {quota_remaining}")
                
                if not has_more:
                    break
                
                page += 1
                time.sleep(0.3)
                
            except Exception as e:
                print(f"  âŒ ì—ëŸ¬: {e}")
                break
        
        print(f"  âœ… '{tag}': {len(all_questions)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_questions
    
    def questions_to_dataframe(self, questions):
        """ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        data = []
        
        for q in questions:
            data.append({
                'question_id': q['question_id'],
                'title': q['title'],
                'tags': '|'.join(q['tags']),
                'view_count': q['view_count'],
                'answer_count': q['answer_count'],
                'score': q['score'],
                'is_answered': q.get('is_answered', False),
                'creation_date': datetime.fromtimestamp(q['creation_date']),
                'owner_type': q['owner'].get('user_type', 'unknown'),
                'link': q['link']
            })
        
        return pd.DataFrame(data)
    
    def collect_multiple_tags(self, tags, from_date, to_date, max_pages=20):
        """ì—¬ëŸ¬ íƒœê·¸ ìˆœì°¨ ìˆ˜ì§‘"""
        all_questions = []
        tag_stats = {}
        
        print("\n" + "="*70)
        print(f"ğŸ“… ê¸°ê°„: {from_date.strftime('%Y-%m-%d')} ~ {to_date.strftime('%Y-%m-%d')}")
        print(f"ğŸ·ï¸  ìˆ˜ì§‘ íƒœê·¸: {len(tags)}ê°œ")
        print("="*70)
        
        for i, tag in enumerate(tags, 1):
            print(f"\n[{i}/{len(tags)}] ", end="")
            questions = self.get_questions_by_tag(tag, from_date, to_date, max_pages)
            
            all_questions.extend(questions)
            tag_stats[tag] = len(questions)
            
            time.sleep(1)  # íƒœê·¸ ê°„ 1ì´ˆ ëŒ€ê¸°
        
        # DataFrame ë³€í™˜
        if all_questions:
            df = pd.DataFrame([q for q in all_questions])
            
            # question_idë¡œ ì¤‘ë³µ ì œê±°
            df_unique = df.drop_duplicates(subset=['question_id'], keep='first')
            
            print("\n" + "="*70)
            print("ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
            print("="*70)
            print(f"ì´ ìˆ˜ì§‘: {len(all_questions)}ê°œ")
            print(f"ì¤‘ë³µ ì œê±° í›„: {len(df_unique)}ê°œ")
            
            print("\nğŸ“Š íƒœê·¸ë³„ ìˆ˜ì§‘ í˜„í™©:")
            for tag, count in sorted(tag_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"  {tag:25s}: {count:4d}ê°œ")
            
            # DataFrame í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            final_df = self.questions_to_dataframe(df_unique.to_dict('records'))
            
            return final_df, tag_stats
        
        return None, tag_stats


# ========== 2023ë…„ AI ê´€ë ¨ ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ì§‘ ==========

collector = ComprehensiveStackOverflowCollector()

# AI ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ (RQ2 ê²€ì¦ìš©)
ai_keywords = [
    # ChatGPT ê´€ë ¨
    "chatgpt",
    "gpt-4",
    "gpt-3.5",
    "gpt-3",
    "openai-api",
    
    # AI Agent ê´€ë ¨
    "ai-agent",
    "autonomous-agent",
    "langchain",
    "autogpt",
    
    # Vibe Coding ê´€ë ¨ (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§)
    "prompt-engineering",
    "prompt-design",
    
    # AI ì½”ë”© ë„êµ¬
    "github-copilot",
    "ai-assisted-coding",
    "code-generation",
    "ai-code-review",
    
    # LLM ê´€ë ¨
    "large-language-model",
    "llm",
    "generative-ai",
    
    # ê¸°íƒ€ AI
    "machine-learning",
    "artificial-intelligence",
    "deep-learning",
    "neural-network"
]

print(f"\nğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ: {len(ai_keywords)}ê°œ í‚¤ì›Œë“œ")
print(f"ğŸ“… ê¸°ê°„: 2023ë…„ ì „ì²´")

# 2023ë…„ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
df_2023, tag_stats = collector.collect_multiple_tags(
    tags=ai_keywords,
    from_date=datetime(2023, 1, 1),
    to_date=datetime(2023, 12, 31),
    max_pages=20
)

# ìƒì„¸ ë¶„ì„
if df_2023 is not None:
    print("\n" + "="*70)
    print("ğŸ“Š ìƒì„¸ í†µê³„")
    print("="*70)
    
    # ê¸°ë³¸ í†µê³„
    print(f"\nì´ ì§ˆë¬¸ ìˆ˜: {len(df_2023)}ê°œ")
    print(f"ë‹µë³€ëœ ì§ˆë¬¸: {df_2023['is_answered'].sum()}ê°œ ({df_2023['is_answered'].sum()/len(df_2023)*100:.1f}%)")
    
    # ì¡°íšŒìˆ˜/ë‹µë³€/ì ìˆ˜ í†µê³„
    print(f"\nì¡°íšŒìˆ˜ í†µê³„:")
    print(f"  í‰ê· : {df_2023['view_count'].mean():.0f}")
    print(f"  ì¤‘ì•™ê°’: {df_2023['view_count'].median():.0f}")
    print(f"  ìµœëŒ€: {df_2023['view_count'].max():,}")
    
    print(f"\në‹µë³€ ìˆ˜ í†µê³„:")
    print(f"  í‰ê· : {df_2023['answer_count'].mean():.1f}")
    print(f"  ìµœëŒ€: {df_2023['answer_count'].max()}")
    
    print(f"\nì ìˆ˜ í†µê³„:")
    print(f"  í‰ê· : {df_2023['score'].mean():.1f}")
    print(f"  ìµœëŒ€: {df_2023['score'].max()}")
    
    # ì›”ë³„ ì§‘ê³„
    df_2023['year_month'] = df_2023['creation_date'].dt.to_period('M')
    monthly_counts = df_2023.groupby('year_month').size()
    
    print(f"\nğŸ“… ì›”ë³„ ì§ˆë¬¸ ìˆ˜:")
    for month, count in monthly_counts.items():
        print(f"  {month}: {count:4d}ê°œ")
    
    # ì£¼ìš” íƒœê·¸ ë¶„ì„
    print(f"\nğŸ·ï¸  ê°€ì¥ ë§ì´ ë“±ì¥í•œ íƒœê·¸ (Top 15):")
    all_tags = []
    for tags_str in df_2023['tags']:
        all_tags.extend(tags_str.split('|'))
    
    tag_counts = pd.Series(all_tags).value_counts()
    for i, (tag, count) in enumerate(tag_counts.head(15).items(), 1):
        print(f"  {i:2d}. {tag:25s}: {count:4d}íšŒ")
    
    # ìƒìœ„ 10ê°œ ì¸ê¸° ì§ˆë¬¸
    print(f"\nâ­ Top 10 ì¸ê¸° ì§ˆë¬¸ (ì¡°íšŒìˆ˜ ê¸°ì¤€):")
    top_10 = df_2023.nlargest(10, 'view_count')[['title', 'view_count', 'answer_count', 'score', 'creation_date']]
    for i, (idx, row) in enumerate(top_10.iterrows(), 1):
        print(f"\n{i:2d}. {row['title'][:70]}")
        print(f"    ğŸ‘ï¸  {row['view_count']:,} ì¡°íšŒ | ğŸ’¬ {row['answer_count']}ê°œ ë‹µë³€ | â­ {row['score']}ì  | ğŸ“… {row['creation_date'].strftime('%Y-%m-%d')}")
    
    # CSV ì €ì¥
    if not os.path.exists('data'):
        os.makedirs('data')
    
    filename = 'data/stackoverflow_ai_2023_full.csv'
    df_2023.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ì „ì²´ ë°ì´í„° ì €ì¥: {filename}")
    
    # ì›”ë³„ í†µê³„ ì €ì¥
    monthly_stats = df_2023.groupby('year_month').agg({
        'question_id': 'count',
        'view_count': 'mean',
        'answer_count': 'mean',
        'score': 'sum'
    }).rename(columns={
        'question_id': 'question_count',
        'view_count': 'avg_views',
        'answer_count': 'avg_answers',
        'score': 'total_score'
    })
    
    monthly_stats.to_csv('data/stackoverflow_ai_2023_monthly.csv')
    print(f"ğŸ’¾ ì›”ë³„ í†µê³„ ì €ì¥: data/stackoverflow_ai_2023_monthly.csv")
    
    # íƒœê·¸ë³„ í†µê³„ ì €ì¥
    tag_stats_df = pd.DataFrame(list(tag_stats.items()), columns=['tag', 'count'])
    tag_stats_df = tag_stats_df.sort_values('count', ascending=False)
    tag_stats_df.to_csv('data/stackoverflow_tag_stats_2023.csv', index=False)
    print(f"ğŸ’¾ íƒœê·¸ë³„ í†µê³„ ì €ì¥: data/stackoverflow_tag_stats_2023.csv")

print("\n" + "="*70)
print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("="*70)